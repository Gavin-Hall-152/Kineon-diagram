{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6685a6cd-5528-420b-859a-270d3373527c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def dim_program_progress(catalog_name):\n",
    "    # Load required source tables\n",
    "    analytics_program_completions = f\"{catalog_name}.cleaned_data.analytics_program_completions\"\n",
    "    analytics_program_overview = f\"{catalog_name}.cleaned_data.analytics_program_overview\"\n",
    "    completions = spark.read.table(analytics_program_completions)\n",
    "    overview = spark.read.table(analytics_program_overview)\n",
    "\n",
    "    # Step 1: Create base 'program_users' dataset with unique composite key\n",
    "    program_users = completions.select(\n",
    "        F.concat_ws(\"_\", F.col(\"User ID\").cast(\"string\"), F.col(\"Program ID\").cast(\"string\")).alias(\"program_user_id_str\"),\n",
    "        F.col(\"Program ID\").alias(\"program_id\"),\n",
    "        F.col(\"User ID\").alias(\"user_id\"),\n",
    "        F.col(\"Status\").alias(\"status\"),\n",
    "        F.col(\"Time assigned\").alias(\"time_assigned\"),\n",
    "        F.col(\"Time completed\").alias(\"time_completed\"),\n",
    "        F.col(\"Time started\").alias(\"time_started\")\n",
    "    )\n",
    "\n",
    "    # Step 2: Identify currently assigned users\n",
    "    currently_assigned = overview.select(\"Program ID\", \"User ID\").distinct() \\\n",
    "                                 .withColumnRenamed(\"Program ID\", \"program_id\") \\\n",
    "                                 .withColumnRenamed(\"User ID\", \"user_id\")\n",
    "\n",
    "    # Step 3: Join and compute final program progress logic\n",
    "    joined = program_users.join(currently_assigned, [\"program_id\", \"user_id\"], how=\"left\")\n",
    "\n",
    "    final_program_progress = joined.withColumn(\n",
    "        \"time_completed\",\n",
    "        F.when(F.col(\"program_id\").isNull(), \n",
    "            F.when(F.col(\"time_completed\").isNull(), F.col(\"time_assigned\"))\n",
    "             .otherwise(F.col(\"time_completed\"))\n",
    "        ).otherwise(F.col(\"time_completed\"))\n",
    "    ).withColumn(\n",
    "        \"assignmentstatus\",\n",
    "        F.when(currently_assigned[\"program_id\"].isNull(), F.lit(\"Unassigned\")).otherwise(F.lit(\"Assigned\"))\n",
    "    )\n",
    "\n",
    "    # Step 4: Add program_user_id using row_number for unique ID\n",
    "    window_spec = Window.orderBy(\"program_user_id_str\")\n",
    "    final_program_progress = final_program_progress.withColumn(\"program_user_id\", F.row_number().over(window_spec)) \\\n",
    "                                                   .select(\n",
    "                                                       \"program_user_id\",\n",
    "                                                       \"program_id\",\n",
    "                                                       \"user_id\",\n",
    "                                                       \"status\",\n",
    "                                                       \"time_assigned\",\n",
    "                                                       \"time_completed\",\n",
    "                                                       \"time_started\",\n",
    "                                                       \"assignmentstatus\"\n",
    "                                                   )\n",
    "\n",
    "    # Step 5: Append synthetic 'N/A' record\n",
    "    na_row = spark.createDataFrame([\n",
    "        (-1, -1, -1, \"N/A\", \"1900-01-01\", \"1900-01-01\", \"1900-01-01\", \"N/A\")\n",
    "    ], [\"program_user_id\", \"program_id\", \"user_id\", \"status\", \"time_assigned\", \"time_completed\", \"time_started\", \"assignmentstatus\"])\n",
    "\n",
    "    result = final_program_progress.unionByName(na_row)\n",
    "\n",
    "    # Save to Delta\n",
    "    output_path = f\"{catalog_name}.schema.dim_program_progress\"\n",
    "    result.write.format(\"delta\").mode(\"overwrite\").saveAsTable(output_path)\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dim_program_progress",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
